## Sandboxes {#sec-sandboxes}

[k8s Sandbox](https://k8s-sandbox.aisi.org.uk/)
:   Python package that provides a Kubernetes sandbox environment for Inspect. <small>*[UK AISI](https://github.com/UKGovernmentBEIS/inspect_k8s_sandbox)*</small>

[EC2 Sandbox](https://github.com/UKGovernmentBEIS/inspect_ec2_sandbox)
:   Python package that provides a EC2 virtual machine sandbox environment for Inspect. <small>*[UK AISI](https://github.com/UKGovernmentBEIS/inspect_ec2_sandbox)*</small>

[Modal Sandbox](https://github.com/meridianlabs-ai/inspect_sandboxes/tree/main/src/inspect_sandboxes/modal)
:   Serverless container sandbox for Inspect using Modal's cloud infrastructure. <small>*[Meridian](https://github.com/meridianlabs-ai/inspect_sandboxes)*</small>

[Proxmox Sandbox](https://github.com/UKGovernmentBEIS/inspect_proxmox_sandbox)
:   Use virtual machines, running within a [Proxmox](https://www.proxmox.com/en/products/proxmox-virtual-environment/overview) instance, as Inspect sandboxes. <small>*[UK AISI](https://github.com/UKGovernmentBEIS/inspect_proxmox_sandbox)*</small>

[Inspect Policy Sandbox](https://github.com/Dedulus/inspect-policy-sandbox)
:   Policy enforced sandbox wrapper for Inspect AI that allows fine grained control over command execution and file I/O without modifying core sandbox backends. <small>*[Arnab Mitra](https://github.com/Dedulus)*</small>


## Analysis {#sec-analysis}

[Inspect Scout](https://meridianlabs-ai.github.io/inspect_scout/)
:   Transcript analysis for Inspect evaluations. <small>*[Meridian](https://github.com/meridianlabs-ai/inspect_scout)*</small>

[Inspect Viz](https://meridianlabs-ai.github.io/inspect_viz/)
:   Interactive data visualization for Inspect evaluations. <small>*[Meridian](https://github.com/meridianlabs-ai/inspect_viz)*</small>

[Docent](https://docs.transluce.org/)
:   Tools to summarize, cluster, and search over agent transcripts. <small>*[Transluce](https://transluce.org/introducing-docent)*</small>

[Lunette](https://docs.lunette.dev)
:   Platform for understanding and improving agents. <small>*[Fulcrum Research](https://fulcrumresearch.ai)*</small>

[Inspect WandB](https://github.com/DanielPolatajko/inspect_wandb)
:   Integration with Weights and Biases platform. <small>*[Arcadia](https://www.arcadiaimpact.org/)*</small>


## Frameworks {#sec-frameworks}

[Inspect SWE](https://meridianlabs-ai.github.io/inspect_swe/)
:   Software engineering agents (Claude Code and Codex CLI) for Inspect. <small>*[Meridian](https://github.com/meridianlabs-ai/inspect_swe)*</small>

[Inspect Cyber](https://ukgovernmentbeis.github.io/inspect_cyber/)
:   Python package that streamlines the process of creating agentic cyber evaluations in Inspect. <small>*[UK AISI](https://github.com/UKGovernmentBEIS/inspect_cyber)*</small>

[Petri](https://safety-research.github.io/petri/)
:   Framework for rapidly testing concrete alignment hypotheses end‑to‑end, including automatic generation of realistic audit scenarios. <small>*[Anthropic](https://www.anthropic.com/research/petri-open-source-auditing)*</small>

[Control Arena](https://github.com/UKGovernmentBEIS/control-arena)
:   Framework for running experiments on AI Control and Monitoring. <small>*[UK AISI](https://github.com/UKGovernmentBEIS/control-arena)*</small>


## Tooling {#sec-tooling}

[Inspect Flow](https://meridianlabs-ai.github.io/inspect_flow/)
:   Workflow orchestration for running Inspect evaluations at scale with repeatability and maintainability. <small>*[Meridian](https://github.com/meridianlabs-ai/inspect_flow)*</small>

[Evaljobs](https://github.com/dvsrepo/evaljobs)
:   Run evals on Hugging Face GPUs and share results and code on the Hugging Face Hub. <small>*[Hugging Face](https://github.com/dvsrepo/evaljobs)*</small>

[Inspect VS Code](https://marketplace.visualstudio.com/items?itemName=ukaisi.inspect-ai)
:   VS Code extension that assists with developing and debugging Inspect evaluations. <small>*[Meridian](https://github.com/meridianlabs-ai/inspect-vscode)*</small>


## Evals {#sec-evals}

[Inspect Evals](https://ukgovernmentbeis.github.io/inspect_evals/)
:   Community-contributed library of LLM evaluations covering safety, coding, reasoning, knowledge, and agent capabilities. <small>*[UK AISI](https://github.com/UKGovernmentBEIS/inspect_evals)*</small>

[OpenBench](https://github.com/groq/openbench)
:   Standardized, reproducible benchmarking for LLMs across 30+ evals. <small>*[Groq](https://github.com/groq)*</small>

[Inspect Harbor](https://github.com/meridianlabs-ai/inspect_harbor)
:   Run Harbor RL tasks with Inspect AI, with access to 40+ registry datasets including terminal-bench, replicationbench, and compilebench. <small>*[Meridian](https://github.com/meridianlabs-ai/inspect_harbor)*</small>

